---
title: "Clustering_WePlant"
format: html
editor: visual
---

# Clustering

```{r}
plant_data <- read.csv2("../data/plant_growth_data.csv",header=TRUE,sep=",")
head(plant_data)
```

```{r}
colnames(plant_data)
```

```{r}
df_cluster <- plant_data[, c("Sunlight_Hours", 
                             "Temperature", 
                             "Humidity")]

# Conversion en numérique (si besoin) 
df_cluster[] <- lapply(df_cluster, function(x) {
  # Si la colonne est factor ou character, la passer en numeric
  as.numeric(as.character(x))
})

df_scaled <- scale(df_cluster)

```

```{r}
set.seed(123)  # Pour la reproductibilité
wss <- c()
for (k in 1:10) {
  km <- kmeans(df_scaled, centers = k, nstart = 25)
  wss[k] <- sum(km$withinss)
}

plot(1:10, wss, type="b", pch=19,
     xlab="Nombre de clusters (k)",
     ylab="Somme des carrés intra-clusters (WithinSS)",
     main="Méthode du coude")

```

```{r}
# Fixer un "seed" pour la reproductibilité
set.seed(123)

k <- 3

# Effectuer le K-means
km_res <- kmeans(df_scaled, centers = k, nstart = 25)

# Récupérer le cluster attribué à chaque ligne (chaque plante)
clusters_km <- km_res$cluster

# Afficher la taille de chaque cluster
table(clusters_km)


plant_data$cluster_km <- clusters_km

# Vérifier le résultat
head(plant_data)
```

```{r}
 # Calcul de la matrice de distances
dist_mat <- dist(df_scaled, method = "euclidean")

# Clustering hiérarchique (méthode de Ward)
hc_res <- hclust(dist_mat, method = "ward.D2")

# Dendrogramme
plot(hc_res, main = "Dendrogramme - Clustering hiérarchique", 
     xlab = "", sub = "")

# Découper l'arbre en 3 clusters (exemple)
rect.hclust(hc_res, k = 3, border = "red")
clusters_hc <- cutree(hc_res, k = 3)

# Ajouter la colonne "cluster_hc" au data frame
# Remplacez "df_plants" par le nom exact de votre data frame 
# (par exemple "df_plants <- plant_data"), 
# puis utilisez df_plants$cluster_hc :
plant_data$cluster_hc <- clusters_hc


```

```{r}
pca_res <- prcomp(df_scaled, scale. = FALSE)

# 2) Construire un data frame avec les coordonnées sur PC1 et PC2,
#    et la variable "cluster" issue de km_res
pca_data <- data.frame(
  PC1 = pca_res$x[, 1],
  PC2 = pca_res$x[, 2],
  cluster = factor(km_res$cluster)  # on factorise pour faciliter la couleur
)

# 3) Visualisation en base R
plot(
  pca_data$PC1, pca_data$PC2,
  col = pca_data$cluster,  # couleur en fonction du cluster
  pch = 19,                # forme de point
  xlab = "PC1",
  ylab = "PC2",
  main = "Visualisation des clusters K-means sur les 2 premières composantes"
)

# 4) Ajouter une légende (optionnel)
legend(
  "topright",
  legend = levels(pca_data$cluster),
  col = 1:length(levels(pca_data$cluster)),
  pch = 19,
  title = "Clusters"
)
```

```{r}
##########################################################
# 1) Algorithme de Lloyd (k-means) - Implémentation maison
#    avec affichage d'informations à chaque itération
##########################################################

# Distance euclidienne
euclid_dist <- function(a, b) {
  sqrt(sum((a - b)^2))
}

# Algorithme de Lloyd
lloyd_kmeans <- function(data, k = 3, max_iter = 100, seed = 123) {
  set.seed(seed)
  
  # Convertir en matrice si data est un data frame
  X <- as.matrix(data)
  
  # Dimensions
  n <- nrow(X)  # nombre d'observations
  d <- ncol(X)  # nombre de variables
  
  # 1) Initialisation : choisir k centres initiaux au hasard
  init_idx <- sample(seq_len(n), size = k, replace = FALSE)
  centers <- X[init_idx, , drop = FALSE]  # matrice k x d
  
  # Vecteur pour stocker l'assignation de chaque point
  clusters <- rep(0, n)
  old_clusters <- rep(-1, n)
  
  iter <- 0
  
  # 2) Boucle principale
  while (iter < max_iter && !all(clusters == old_clusters)) {
    iter <- iter + 1
    
    # a) Assignation : chaque point -> centre le plus proche
    old_clusters <- clusters
    for (i in 1:n) {
      dist_vec <- sapply(1:k, function(c) euclid_dist(X[i, ], centers[c, ]))
      clusters[i] <- which.min(dist_vec)
    }
    
    # b) Mise à jour des centres
    for (c in 1:k) {
      pts_cluster_c <- X[clusters == c, , drop = FALSE]
      if (nrow(pts_cluster_c) > 0) {
        centers[c, ] <- colMeans(pts_cluster_c)
      }
    }
    
    # c) Calculer un indicateur de SSE (somme des carrés intra-cluster)
    SSE <- 0
    for (c in 1:k) {
      pts_cluster_c <- X[clusters == c, , drop = FALSE]
      if (nrow(pts_cluster_c) > 0) {
        # Distance au centre
        dists <- apply(pts_cluster_c, 1, function(pt) euclid_dist(pt, centers[c, ]))
        SSE <- SSE + sum(dists^2)
      }
    }
    
    # d) Calculer le nombre de changements de cluster
    changes <- sum(clusters != old_clusters)
    
    # e) Affichage du numéro d'itération et des indicateurs
    cat(sprintf("Iteration %d | SSE = %.2f | Changements = %d\n",
                iter, SSE, changes))
  }
  
  # Résultat
  list(
    clusters = clusters,
    centers = centers,
    iterations = iter
  )
}

##########################################################
# 2) Exécution de l'algorithme sur vos données
##########################################################

# Supposez que df_scaled est un data frame/matrice numérique
# déjà standardisé (moyenne=0, écart-type=1).
# On fixe k=3.

result_km <- lloyd_kmeans(df_scaled, k = 3, max_iter = 100, seed = 123)

# Les clusters attribués à chaque observation
head(result_km$clusters)

# Les centres finaux
result_km$centers

# Nombre total d'itérations
result_km$iterations

##########################################################
# 3) Visualisation (projection en PCA sur PC1-PC2)
##########################################################

# a) Calcul de la PCA
pca_res <- prcomp(df_scaled, scale. = FALSE) 
# (df_scaled est déjà centré-réduit)

# b) Préparer un data frame pour tracer les points
pca_data <- data.frame(
  PC1 = pca_res$x[, 1],
  PC2 = pca_res$x[, 2],
  cluster = factor(result_km$clusters)  # pour la couleur
)

# c) Projeter les centres dans le plan PC1-PC2
centers_pca <- predict(pca_res, newdata = result_km$centers)

# d) Tracer le nuage de points
plot(
  pca_data$PC1, pca_data$PC2,
  col = pca_data$cluster,  # couleur en fonction du cluster
  pch = 19,
  xlab = "PC1",
  ylab = "PC2",
  main = "Clusters (Algorithme de Lloyd) - Plan PC1 vs PC2"
)

# e) Ajouter les centres (avec un symbole distinct, pch=8)
points(
  centers_pca[, 1], centers_pca[, 2],
  pch = 8, cex = 2, lwd = 2,
  col = 1:nrow(centers_pca)
)

# f) Ajouter une légende (optionnel)
legend(
  "topright",
  legend = paste("Cluster", 1:3),
  col = 1:3,
  pch = 19,
  title = "Clusters"
)



```
