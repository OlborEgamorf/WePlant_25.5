---
title: "Clustering_WePlant"
format: html
editor: visual
---

# Clustering

```{r}
install.packages("readxl")  # si pas déjà installé
library(readxl)
```

```{r}
plant_data <- read.csv2("../data/plant_moniter_health_data.csv",header=TRUE,sep=",")
head(plant_data)
```

```{r}

df_cluster <- plant_data[, c("Temperature_C", 
                             "Humidity_.", 
                             "Soil_Moisture_.", 
                             "Soil_pH", 
                             "Nutrient_Level", 
                             "Light_Intensity_lux", 
                             "Health_Score")]

# Conversion en numérique (si besoin) 
df_cluster[] <- lapply(df_cluster, function(x) {
  # Si la colonne est factor ou character, la passer en numeric
  as.numeric(as.character(x))
})

df_scaled <- scale(df_cluster)


```

```{r}
set.seed(123)  # Pour la reproductibilité
wss <- c()
for (k in 1:10) {
  km <- kmeans(df_scaled, centers = k, nstart = 25)
  wss[k] <- sum(km$withinss)
}

plot(1:10, wss, type="b", pch=19,
     xlab="Nombre de clusters (k)",
     ylab="Somme des carrés intra-clusters (WithinSS)",
     main="Méthode du coude")

```

```{r}
# Fixer un "seed" pour la reproductibilité
set.seed(123)

k <- 3

# Effectuer le K-means
km_res <- kmeans(df_scaled, centers = k, nstart = 25)

# Récupérer le cluster attribué à chaque ligne (chaque plante)
clusters_km <- km_res$cluster

# Afficher la taille de chaque cluster
table(clusters_km)


plant_data$cluster_km <- clusters_km

# Vérifier le résultat
head(plant_data)

```

```{r}
 # Calcul de la matrice de distances
dist_mat <- dist(df_scaled, method = "euclidean")

# Clustering hiérarchique (méthode de Ward)
hc_res <- hclust(dist_mat, method = "ward.D2")

# Dendrogramme
plot(hc_res, main = "Dendrogramme - Clustering hiérarchique", 
     xlab = "", sub = "")

# Découper l'arbre en 3 clusters (exemple)
rect.hclust(hc_res, k = 3, border = "red")
clusters_hc <- cutree(hc_res, k = 3)

plant_data$cluster_hc <- clusters_hc


```

```{r}

pca_res <- prcomp(df_scaled, scale. = FALSE)

# 2) Construire un data frame avec les coordonnées sur PC1 et PC2,
#    et la variable "cluster" issue de km_res
pca_data <- data.frame(
  PC1 = pca_res$x[, 1],
  PC2 = pca_res$x[, 2],
  cluster = factor(km_res$cluster)  # on factorise pour faciliter la couleur
)

# 3) Visualisation en base R
plot(
  pca_data$PC1, pca_data$PC2,
  col = pca_data$cluster,  # couleur en fonction du cluster
  pch = 19,                # forme de point
  xlab = "PC1",
  ylab = "PC2",
  main = "Visualisation des clusters K-means sur les 2 premières composantes"
)

# 4) Ajouter une légende (optionnel)
legend(
  "topright",
  legend = levels(pca_data$cluster),
  col = 1:length(levels(pca_data$cluster)),
  pch = 19,
  title = "Clusters"
)

```

```{r}
pca_res <- prcomp(df_scaled, scale. = FALSE)
# (df_scaled est déjà centré-réduit, donc scale.=FALSE ici)

pca_data <- data.frame(
  PC1 = pca_res$x[, 1],
  PC2 = pca_res$x[, 2],
  PC3 = pca_res$x[, 3],
  cluster = factor(km_res$cluster)  # km_res$cluster = numéros de cluster
)

# On crée une grille 1 x 3 pour afficher 3 graphiques côte à côte
par(mfrow = c(1, 3))

# --- Graphique 1 : PC1 vs PC2
plot(pca_data$PC1, pca_data$PC2,
     col = pca_data$cluster, pch = 19,
     xlab = "PC1", ylab = "PC2",
     main = "Clusters : PC1 vs PC2")

# --- Graphique 2 : PC1 vs PC3
plot(pca_data$PC1, pca_data$PC3,
     col = pca_data$cluster, pch = 19,
     xlab = "PC1", ylab = "PC3",
     main = "Clusters : PC1 vs PC3")

# --- Graphique 3 : PC2 vs PC3
plot(pca_data$PC2, pca_data$PC3,
     col = pca_data$cluster, pch = 19,
     xlab = "PC2", ylab = "PC3",
     main = "Clusters : PC2 vs PC3")


legend("topright",
       legend = levels(pca_data$cluster),
       col = 1:length(levels(pca_data$cluster)),
       pch = 19,
       title = "Clusters")

```

```{r}
# 1) Effectuer la PCA
pca_res <- prcomp(df_scaled, scale. = TRUE) 
# (Si df_scaled est déjà centré-réduit, vous pouvez mettre scale. = FALSE)

# 2) Extraire les loadings (coefficient de chaque variable dans chaque composante)
loadings <- pca_res$rotation
# C'est une matrice de taille (nombre_de_variables x nombre_de_composantes)

# 3) Calculer les loadings au carré
loadings_sq <- loadings^2

# 4) Calculer la contribution en pourcentage de chaque variable à chaque composante
var_contrib <- sweep(loadings_sq, 2, colSums(loadings_sq), FUN = "/") * 100

# 5) Afficher la table de contributions
var_contrib

```

```{r}
# Distance euclidienne
euclid_dist <- function(a, b) {
  sqrt(sum((a - b)^2))
}

# Algorithme de Lloyd
lloyd_kmeans <- function(data, k = 3, max_iter = 100, seed = 123) {
  set.seed(seed)
  
  # Convertir en matrice si data est un data frame
  X <- as.matrix(data)
  
  # Dimensions
  n <- nrow(X)  # nombre d'observations
  d <- ncol(X)  # nombre de variables
  
  # 1) Initialisation : choisir k centres initiaux au hasard
  init_idx <- sample(seq_len(n), size = k, replace = FALSE)
  centers <- X[init_idx, , drop = FALSE]  # matrice k x d
  
  # Vecteur pour stocker l'assignation de chaque point
  clusters <- rep(0, n)
  old_clusters <- rep(-1, n)
  
  iter <- 0
  
  # 2) Boucle principale
  while (iter < max_iter && !all(clusters == old_clusters)) {
    iter <- iter + 1
    
    # --> Affichage du numéro d'itération
    cat("Iteration:", iter, "\n")
    
    old_clusters <- clusters
    
    # a) Assignation : chaque point -> centre le plus proche
    for (i in 1:n) {
      dist_vec <- sapply(1:k, function(c) euclid_dist(X[i, ], centers[c, ]))
      clusters[i] <- which.min(dist_vec)
    }
    
    # b) Mise à jour des centres
    for (c in 1:k) {
      pts_cluster_c <- X[clusters == c, , drop = FALSE]
      if (nrow(pts_cluster_c) > 0) {
        centers[c, ] <- colMeans(pts_cluster_c)
      }
    }
  }
  
  # Résultat
  list(
    clusters = clusters,
    centers = centers,
    iterations = iter
  )
}

result_km <- lloyd_kmeans(df_scaled, k = 3, max_iter = 100, seed = 123)

# Les clusters attribués à chaque observation
head(result_km$clusters)

# Les centres finaux
result_km$centers

# Nombre total d'itérations
result_km$iterations


# a) Calcul de la PCA
pca_res <- prcomp(df_scaled, scale. = FALSE) 
# (df_scaled est déjà centré-réduit)

# b) Préparer un data frame pour tracer les points
pca_data <- data.frame(
  PC1 = pca_res$x[, 1],
  PC2 = pca_res$x[, 2],
  cluster = factor(result_km$clusters)  # pour la couleur
)

# c) Projeter les centres dans le plan PC1-PC2
centers_pca <- predict(pca_res, newdata = result_km$centers)

# d) Tracer le nuage de points
plot(
  pca_data$PC1, pca_data$PC2,
  col = pca_data$cluster,  # couleur en fonction du cluster
  pch = 19,
  xlab = "PC1",
  ylab = "PC2",
  main = "Clusters (Algorithme de Lloyd) - Plan PC1 vs PC2"
)

# e) Ajouter les centres (avec un symbole distinct, pch=8)
points(
  centers_pca[, 1], centers_pca[, 2],
  pch = 8, cex = 2, lwd = 2,
  col = 1:nrow(centers_pca)
)

# f) Ajouter une légende (optionnel)
legend(
  "topright",
  legend = paste("Cluster", 1:3),
  col = 1:3,
  pch = 19,
  title = "Clusters"
)

```

```{r}
write.csv(plant_data, file = "plantes_clusters.csv", row.names = FALSE)

```
